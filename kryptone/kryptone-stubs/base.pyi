import datetime
import time
from typing import List, Literal, NamedTuple, NoReturn, Union
from urllib.parse import ParseResult

from selenium.webdriver import Chrome, Edge
from selenium.webdriver.remote.webelement import WebElement

from kryptone.mixins import EmailMixin, SEOMixin
from kryptone.routing import Router
from kryptone.utils.file_readers import URLCache
from kryptone.utils.urls import URL, UrlPassesRegexTest, URLPassesTest

WEBDRIVER_ENVIRONMENT_PATH: str = 'KRYPTONE_WEBDRIVER'

def get_selenium_browser_instance(
    browser_name: str = ...
) -> Union[Edge, Chrome]: ...


class CrawlerOptions:
    spider: type[Union[SiteCrawler, SinglePageAutomater]] = ...
    spider_name: str = ...
    verbose_name: str = ...
    initial_spider_meta: type = ...
    domains: list[str] = ...
    audit_page: bool = ...
    url_passes_tests: Union[URLPassesTest, UrlPassesRegexTest] = ...
    debug_mode: bool = ...
    site_language: Literal['en'] = ...
    default_scroll_step: Literal[80] = ...
    gather_emails: bool = ...
    router: Router = ...

    def __repr__(self) -> str: ...
    def add_meta_options(self, options: tuple) -> NoReturn: ...
    def prepare(self) -> NoReturn: ...


class Crawler(type):
    def __new__(cls, name: str, bases: tuple, attrs: dict) -> type: ...
    def prepare(cls: type) -> NoReturn: ...


class BaseCrawler(metaclass=Crawler):
    urls_to_visit: set[str] = ...
    visited_urls: set[str] = ...
    list_of_seen_urls: set[str] = ...
    browser_name: str = ...
    debug_mode: bool = ...
    timezone: str = 'UTC'
    default_scroll_step:  int = 80

    class Meta:
        ...

    def __repr__(self) -> str: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def get_page_link_elements(self) -> List[WebElement]: ...
    @property
    def completion_percentage(self) -> int: ...
    @property
    def name(self) -> str: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def get_page_link_elements(self) -> List[WebElement]: ...
    @property
    def get_title_element(self) -> WebElement: ...
    def _backup_urls(self) -> NoReturn: ...
    def urljoin(self, path) -> str: ...

    def create_filename(
        self,
        length: int = ...,
        extension: str = ...
    ) -> str: ...
    def build_headers(self, options: dict) -> NoReturn: ...
    def run_filters(self) -> Union[list, set]: ...
    def add_urls(self, *urls_or_paths) -> NoReturn: ...
    def get_page_urls(self): ...

    def scroll_window(
        self,
        wait_time: int = ...,
        increment: int = ...,
        stop_at: int = ...
    ) -> NoReturn: ...

    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...
    ) -> NoReturn: ...

    def evaluate_xpath(self, path: str) -> NoReturn: ...

    def scroll_page_section(
        self,
        xpath: str = ...,
        css_selector: str = ...
    ): ...

    def calculate_performance(self) -> NoReturn: ...
    def get_current_date(self) -> datetime.datetime: ...
    def post_visit_actions(self, **kwargs): ...
    def run_actions(self, current_url: URL, **kwargs) -> NoReturn: ...
    def create_dump(self) -> NoReturn: ...


class SiteCrawler(SEOMixin, EmailMixin, BaseCrawler):
    start_url: str = ...
    start_xml_url: str = ...
    _start_url_object: ParseResult = ...
    _start_date: datetime.datetime = ...
    _start_time: time.time
    _meta: CrawlerOptions = ...
    driver: Union[Edge, Chrome] = ...
    performance_audit: NamedTuple = ...

    def __init__(self, browser_name: str = ...) -> NoReturn: ...
    def resume(self, **kwargs): ...
    def start_from_sitemap_xml(self, url: str, **kwargs): ...
    def start_from_html_sitemap(self, url: str, **kwargs): ...

    def start(
        self,
        start_urls: List[str] = ...,
        url_cache: URLCache = ...,
        **kwargs
    ) -> NoReturn: ...


class SinglePageAutomater(BaseCrawler):
    ...
