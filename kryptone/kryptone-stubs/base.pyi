import datetime
import time
from typing import (Coroutine, Iterator, List, Literal, NamedTuple, NoReturn,
                    Tuple, Union)
from urllib.parse import ParseResult
from functools import cached_property
import pandas
from selenium.webdriver import Chrome, Edge
from selenium.webdriver.remote.webelement import WebElement

from kryptone.mixins import EmailMixin, SEOMixin
from kryptone.routing import Router
from kryptone.utils.file_readers import LoadStartUrls, URLCache
from kryptone.utils.iterators import AsyncIterator
from kryptone.utils.urls import URL, UrlIgnoreRegexTest, URLPassesRegexTest, URLIgnoreTest

WEBDRIVER_ENVIRONMENT_PATH: str = 'KRYPTONE_WEBDRIVER'


def get_selenium_browser_instance(
    browser_name: str = ...,
    headless: bool = ...,
    load_images: bool = ...,
    load_js: bool = ...
) -> Union[Edge, Chrome]: ...


class PerformanceAudit(NamedTuple):
    days: int
    duration: int


class URLsAudit(NamedTuple):
    count_urls_to_visit: int
    count_visited_urls: int
    completion_percentage: float
    total_urls: int


class CrawlerOptions:
    spider: SiteCrawler = ...
    spider_name: str = ...
    verbose_name: str = ...
    initial_spider_meta: type = ...
    domains: list[str] = ...
    audit_page: bool = ...
    url_passes_tests: Union[UrlIgnoreRegexTest,
                            URLPassesRegexTest, URLIgnoreTest] = ...
    debug_mode: bool = ...
    site_language: Literal['en'] = ...
    default_scroll_step: Literal[80] = ...
    gather_emails: bool = ...
    router: Router = ...
    crawl: bool = ...
    start_urls: Union[LoadStartUrls, list[str]]
    restrict_search_to: list[str] = ...
    ignore_queries: bool = ...
    ignore_images: bool = ...

    def __repr__(self) -> str: ...
    def add_meta_options(self, options: tuple) -> None: ...
    def prepare(self) -> None: ...


class Crawler(type):
    def __new__(cls: type, name: str, bases: tuple, attrs: dict) -> type: ...
    def prepare(cls: type) -> None: ...


class BaseCrawler(metaclass=Crawler):
    urls_to_visit: set[str] = ...
    visited_urls: set[str] = ...
    list_of_seen_urls: set[str] = ...
    browser_name: str = ...
    debug_mode: bool = ...
    timezone: str = 'UTC'
    default_scroll_step:  int = 80

    class Meta:
        ...

    def __init__(self, browser_name: str = ...): ...
    def __repr__(self) -> str: ...
    @property
    def get_page_link_elements(self) -> List[str]: ...
    @property
    def get_title_element(self) -> WebElement: ...
    @property
    def get_origin(self) -> str: ...
    @cached_property
    def default_image_extensions(self) -> list[str]: ...
    @property
    def name(self) -> str: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def calculate_performance(self) -> int: ...
    def _backup_urls(self) -> None: ...
    def urljoin(self, path: str) -> str: ...
    def run_filters(self) -> None: ...
    def get_page_urls(self) -> None: ...
    def add_urls(self, *urls_or_paths) -> None: ...
    def build_headers(self, options: dict) -> None: ...
    def run_filters(self) -> Union[list, set]: ...
    def refresh_page_urls(self) -> None: ...

    def scroll_window(
        self,
        wait_time: int = ...,
        increment: int = ...,
        stop_at: int = ...
    ) -> None: ...

    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...,
        wait_time: int = ...
    ) -> None: ...

    def evaluate_xpath(self, path: str) -> None: ...

    def scroll_page_section(
        self,
        xpath: str = ...,
        css_selector: str = ...
    ): ...

    def calculate_performance(self) -> Tuple[PerformanceAudit, URLsAudit]: ...
    def post_visit_actions(self, **kwargs): ...
    def run_actions(self, current_url: URL, **kwargs) -> None: ...
    def create_dump(self) -> None: ...


class SiteCrawler(SEOMixin, EmailMixin, BaseCrawler):
    start_url: str = ...
    _start_url_object: ParseResult = ...
    _start_date: datetime.datetime = ...
    _end_date: datetime.datetime = ...
    _start_time: time.time
    _meta: CrawlerOptions = ...
    driver: Union[Edge, Chrome] = ...
    performance_audit: PerformanceAudit = ...
    urls_audit: URLsAudit = ...
    statistics: dict = ...
    visited_pages_count: int = ...

    def __init__(self, browser_name: str = ...) -> None: ...
    def __del__(self) -> None: ...
    def resume(self, **kwargs): ...
    def start_from_sitemap_xml(self, url: str, **kwargs): ...
    def start_from_html_sitemap(self, url: str, **kwargs): ...

    def start(
        self,
        start_urls: List[str] = ...,
        **kwargs
    ) -> None: ...


class JSONCrawler:
    base_url: str = ...
    receveived_data: List[list, dict] = ...
    iterator = AsyncIterator
    chunks: int = Literal[10]
    request_sent: int = Literal[0]
    max_pages: int = Literal[0]
    current_page_key: str = ...
    current_page: int = Literal[1]
    max_pages_key: int = ...
    paginate_data: bool = ...
    pagination: int = Literal[0]
    _url: URL = ...

    def __init__(self, chunks: int = ...): ...
    @property
    def data(self) -> List[Iterator[list, dict]]: ...

    async def clean(
        self,
        dataframe: pandas.DataFrame
    ) -> Coroutine[pandas.DataFrame]: ...
    async def start(self, interval: int = ...) -> Coroutine: ...
