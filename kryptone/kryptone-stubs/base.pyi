import dataclasses
import datetime
import time
from typing import (Any, Coroutine, DefaultDict, Iterator, List, Literal,
                    OrderedDict, Union, override)
from urllib.parse import ParseResult
from urllib.robotparser import RobotFileParser

import pandas
from selenium.webdriver import Chrome, Edge
from selenium.webdriver.remote.webelement import WebElement

from kryptone.routing import Router
from kryptone.utils.iterators import AsyncIterator
from kryptone.utils.urls import (URL, LoadStartUrls, URLIgnoreRegexTest,
                                 URLIgnoreTest, URLPaginationGenerator,
                                 URLQueryGenerator)

def get_selenium_browser_instance(
    browser_name: str = ...,
    headless: bool = ...,
    load_images: bool = ...,
    load_js: bool = ...
) -> Union[Edge, Chrome]: ...


@dataclasses.dataclass
class PerformanceAudit:
    days: int = 0
    duration: float = 0
    count_urls_to_visit: int = 0
    count_visited_urls: int = 0
    completion_percentage: float = 0
    visited_pages_count: int = 0

    @property
    def total_urls(self) -> int: ...

    def calculate_completion_percentage(self) -> float: ...
    def json(self) -> dict: ...


class CrawlerOptions:
    spider: SiteCrawler = ...
    spider_name: str = ...
    verbose_name: str = ...
    initial_spider_meta: type = ...
    domains: list[str] = ...
    url_ignore_tests: Union[URLIgnoreRegexTest, URLIgnoreTest] = ...
    debug_mode: bool = ...
    default_scroll_step: Literal[80] = ...
    router: Router = ...
    crawl: bool = ...
    start_urls: Union[
        LoadStartUrls,
        URLPaginationGenerator,
        URLQueryGenerator,
        list[str]
    ]
    restrict_search_to: list[str] = ...
    ignore_queries: bool = ...
    ignore_images: bool = ...
    url_gather_ignore_tests: list[str] = ...
    url_rule_tests: list[str] = ...

    def __repr__(self) -> str: ...

    @property
    def has_start_urls(self) -> bool: ...

    def add_meta_options(self, options: tuple) -> None: ...
    def prepare(self) -> None: ...


class Crawler(type):
    def __new__(cls: type, name: str, bases: tuple, attrs: dict) -> type: ...
    def prepare(cls: type) -> None: ...


class BaseCrawler(metaclass=Crawler):
    urls_to_visit: set[str] = ...
    visited_urls: set[str] = ...
    visited_pages_count: int = ...
    list_of_seen_urls: set[str] = ...
    browser_name: str = ...
    timezone: Literal['UTC'] = ...
    default_scroll_step:  Literal[80] = ...
    driver: Union[Edge, Chrome] = ...
    url_distribution: DefaultDict[list] = ...
    _start_url_object: ParseResult = ...

    class Meta:
        ...

    def __init__(self, browser_name: str = ...): ...
    def __repr__(self) -> str: ...
    @property
    def get_page_link_elements(self) -> List[str]: ...
    @property
    def get_title_element(self) -> WebElement: ...
    @property
    def get_origin(self) -> str: ...
    def _backup_urls(self) -> None: ...
    def _get_robot_txt_parser(self) -> RobotFileParser: ...
    def urljoin(self, path: str) -> str: ...
    def url_structural_check(self, url: str) -> Union[str, ParseResult]: ...

    def url_filters(
        self,
        valid_urls: list[str]
    ) -> set[str]: ...

    def url_rule_test_filter(
        self,
        valid_urls: list[str]
    ) -> set[str]: ...

    def add_urls(self, *urls_or_paths: str) -> None: ...
    def get_page_urls(self, current_url: URL, refresh: bool = ...) -> None: ...

    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...,
        before_click_wait_time: int = ...,
        wait_time: int = ...
    ) -> None: ...

    def calculate_performance(self) -> None: ...
    def post_navigation_actions(self, current_url: URL, **kwargs) -> None: ...
    def before_next_page_actions(self, current_url: URL, **kwargs) -> None: ...

    def current_page_actions(
        self,
        current_url: URL,
        current_json_object: pandas.DataFrame = ...,
        **kwargs
    ) -> None: ...
    def after_fail(self) -> None: ...


@dataclasses.dataclass
class PerformanceAudit:
    days: int = None
    duration: float = None
    count_urls_to_visit: int = 0
    count_visited_urls: int = 0
    completion_percentage: float = 0
    total_urls: int = 0

    @property
    def total_urls(self) -> int: ...

    def calculate_completion_percentage(self) -> None: ...
    def json(self) -> OrderedDict[str, Union[int, float]]: ...


class SiteCrawler(BaseCrawler):
    _start_date: datetime.datetime = ...
    _start_time: time.time = ...
    _end_date: datetime.datetime = ...
    _meta: CrawlerOptions = ...
    start_url: str = ...
    current_iteration: Literal[0] = ...
    performance_audit: PerformanceAudit = ...

    @override
    def __init__(self, browser_name: str = ...) -> None: ...
    def __del__(self) -> None: ...

    @classmethod
    def create(cls, **params: Any) -> SiteCrawler: ...

    def before_start(self, start_urls: list[str], **kwargs) -> None: ...
    def resume(self, windows: int = ..., **kwargs) -> None: ...
    def start_from_sitemap_xml(self, url: str, **kwargs) -> None: ...
    def start_from_html_sitemap(self, url: str, **kwargs) -> None: ...

    def start(
        self,
        start_urls: List[str] = ...,
        **kwargs
    ) -> None: ...

    def boost_start(
        self,
        start_urls: list[str],
        *,
        windows: int = ...,
        **kwargs
    ) -> None: ...


class JSONCrawler:
    base_url: str = ...
    receveived_data: List[list, dict[str, Any]] = ...
    date_sorted_data: DefaultDict[list] = ...
    iterator: AsyncIterator = ...
    chunks: int = Literal[10]
    request_sent: int = Literal[0]
    max_pages: int = Literal[0]
    current_page_key: str = ...
    current_page: int = Literal[1]
    max_pages_key: int = ...
    paginate_data: bool = ...
    pagination: int = Literal[0]
    _url: URL = ...

    def __init__(self, chunks: int = ...): ...

    @property
    def data(self) -> List[Iterator[list, dict]]: ...

    async def after_fail(self) -> Coroutine[None, None, None]: ...

    async def clean(
        self,
        data: Union[list, dict[str, Any]]
    ) -> Coroutine[None, None, pandas.DataFrame]: ...

    async def start(
        self, interval: int = ...
    ) -> Coroutine[None, None, None]: ...
