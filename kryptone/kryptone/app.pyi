from typing import Literal, NoReturn, Union
from urllib.parse import ParseResult

from selenium.webdriver import Chrome, Edge

from kryptone.mixins import EmailMixin, SEOMixin

WEBDRIVER_ENVIRONMENT_PATH = Literal['KRYPTONE_WEBDRIVER']


def get_selenium_browser_instance(
    executable_path: str = ...
) -> Union[Edge, Chrome]: ...


class ActionsMixin:
    default_scroll_step: int = ...

    @property
    def scrolled_to_bottom(self) -> bool: ...
    def scroll_page(self, pixels: int = 2000) -> NoReturn: ...
    def scroll_to(self, percentage: int = 80) -> NoReturn: ...
    def scroll_window(self, pixels: int = 2000) -> NoReturn: ...

    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...
    ) -> NoReturn: ...

    def _test_scroll_page(
        self, 
        xpath: str = ...,
        css_selector: str = ...
    ) -> str: ...


class CrawlerMixin(ActionsMixin, SEOMixin, EmailMixin):
    urls_to_visit: set = ...
    visited_urls: set = ...
    webdriver: Union[Edge, Chrome] = ...
    debug_mode: False = ...
    _start_url_object: ParseResult = ...

    def __init__(self) -> NoReturn: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def get_page_link_elements(self) -> list: ...
    @property
    def completion_percentage(self) -> int: ...
    @property
    def name(self) -> Literal['crawler', 'automation']: ...
    def _backup_urls(self) -> NoReturn: ...
    def post_visit_actions(self, **kwargs) -> NoReturn: ...
    def run_actions(self, current_url: str, **kwargs) -> NoReturn: ...
    def create_dump(self) -> NoReturn: ...


class BaseCrawler(CrawlerMixin):
    start_url: str = ...
    url_validators: list = ...
    url_filters: list = ...

    def get_filename(self, length: int = ..., extension: str = ...) -> str: ...
    def build_headers(self, options: dict) -> NoReturn: ...
    def run_filters(self, exclude: bool = ...) -> list: ...
    def get_page_urls(self, same_domain: bool = ...) -> NoReturn: ...
    def resume(self, **kwargs) -> NoReturn: ...
    def start_from_sitemap_xml(self, url: str, **kwargs) -> NoReturn: ...
    def start_from_html_sitemap(self, url: str, **kwargs) -> NoReturn: ...

    def start(
        self,
        start_urls: list = [],
        debug_mode: bool = ...,
        wait_time: int = ...,
        run_audit: bool = ...,
        language: str = ...
    ) -> NoReturn: ...


class SinglePageAutomater(CrawlerMixin):
    start_urls: list = []

    def start(
        self,
        start_urls: list = [],
        wait_time: int = ...,
        debug_mode: bool = ...
    ) -> NoReturn: ...
