from typing import Literal, NoReturn, Union
from urllib.parse import ParseResult
import datetime
from selenium.webdriver import Chrome, Edge
from selenium.webdriver.remote.webelement import WebElement
from kryptone.mixins import EmailMixin, SEOMixin

WEBDRIVER_ENVIRONMENT_PATH: str['KRYPTONE_WEBDRIVER'] = ...


def get_selenium_browser_instance(
    browser_name: str = ...
) -> Union[Edge, Chrome]: ...


class ActionsMixin:
    default_scroll_step: int = ...

    def scroll_window(
        self,
        wait_time: int = 5,
        increment: int = 1000,
        stop_at: int = None
    ) -> NoReturn: ...
    def save_to_local_storage(self, name: str, data: dict) -> NoReturn: ...

    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...
    ) -> NoReturn: ...
    def evaluate_xpath(self, path: str) -> NoReturn: ...

    def _test_scroll_page(
        self,
        xpath: str = ...,
        css_selector: str = ...
    ) -> str: ...


class CrawlerMixin(ActionsMixin, SEOMixin, EmailMixin):
    urls_to_visit: set = ...
    visited_urls: set = ...
    browser_name: Union['Edge', 'Chrome', 'Firefox'] = ...
    debug_mode: False = ...
    _start_url_object: ParseResult = ...
    driver: Union[Edge, Chrome] = ...

    def __init__(self, browser_name=Union['Edge', 'Chrome']) -> NoReturn: ...
    @property
    def get_html_page_content(self) -> str: ...
    @property
    def get_page_link_elements(self) -> list[WebElement]: ...
    @property
    def completion_percentage(self) -> float: ...
    @property
    def name(self) -> Literal['crawler', 'automation']: ...
    def _backup_urls(self) -> NoReturn: ...
    def get_current_date(self) -> datetime.datetime:
    def post_visit_actions(self, **kwargs) -> NoReturn: ...
    def run_actions(self, current_url: str, **kwargs) -> NoReturn: ...
    def create_dump(self) -> NoReturn: ...


class BaseCrawler(CrawlerMixin):
    start_url: str = ...
    start_xml_url: str = ...
    url_filters: list = ...

    def get_filename(self, length: int = ..., extension: str = ...) -> str: ...
    def build_headers(self, options: dict) -> NoReturn: ...
    def run_filters(self, exclude: bool = ...) -> list: ...
    def add_urls(self, *urls_or_paths) -> NoReturn: ...
    def get_page_urls(self, same_domain: bool = ...) -> NoReturn: ...
    def resume(self, **kwargs) -> NoReturn: ...
    def start_from_sitemap_xml(self, url: str, **kwargs) -> NoReturn: ...
    def start_from_html_sitemap(self, url: str, **kwargs) -> NoReturn: ...

    def start(
        self,
        start_urls: list = [],
        debug_mode: bool = ...,
        wait_time: int = ...,
        run_audit: bool = ...,
        language: str = ...
    ) -> NoReturn: ...


class SinglePageAutomater(CrawlerMixin):
    start_urls: list = []

    def start(
        self,
        start_urls: list = [],
        wait_time: int = ...,
        debug_mode: bool = ...
    ) -> NoReturn: ...
